{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import EncoderDecoderModel\n",
    "\n",
    "# model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\", \"gpt2\")\n",
    "# encoder = model.encoder\n",
    "# decoder = model.decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import T5Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"stories.csv\")\n",
    "# Generate input-output pairs\n",
    "inputs = [\"Guess the main chaaracter name of the following story:\\n\" + row.story for _, row in df.iterrows()]\n",
    "outputs = df['name'].tolist()\n",
    "\n",
    "# Create a pandas DataFrame for easy conversion\n",
    "df_preprocessed = pd.DataFrame({'input_text': inputs, 'output_text': outputs})\n",
    "\n",
    "dataset = Dataset.from_pandas(df_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c30d6c7da24af8aa7c8930b445858a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 't5-small'  # You can change this to any T5 variant (t5-base, t5-large, etc.)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize both inputs and outputs\n",
    "    inputs = tokenizer(examples['input_text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    outputs = tokenizer(examples['output_text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    inputs['labels'] = outputs['input_ids']  # Use the tokenized output as labels for the model\n",
    "    return inputs\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_dataset = dataset.shuffle(seed=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Freeze encoder weights\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Initialize Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",            # output directory where model checkpoints will be saved\n",
    "    evaluation_strategy=\"steps\",       # Evaluate every epoch\n",
    "    learning_rate=5e-5,                # learning rate\n",
    "    per_device_train_batch_size=8,     # batch size for training\n",
    "    per_device_eval_batch_size=8,      # batch size for evaluation\n",
    "    num_train_epochs=3,                # number of training epochs\n",
    "    weight_decay=0.01,                 # strength of weight decay\n",
    "    logging_dir=\"./logs\",              # directory for storing logs\n",
    "    logging_steps=200,                 # log every 200 steps\n",
    "    save_steps=2000,                    # save model checkpoints every 500 steps\n",
    "    load_best_model_at_end=True,       # Load the best model when finished training\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the model to be trained\n",
    "    args=training_args,                  # training arguments\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    # eval_dataset=val_dataset,            # evaluation dataset\n",
    "    tokenizer=tokenizer,                 # tokenizer for preprocessing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdfcf935128f4333a5d51c24465973b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/189 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 54.1821, 'train_samples_per_second': 27.684, 'train_steps_per_second': 3.488, 'train_loss': 4.199990893167163, 'epoch': 3.0}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Trainer: evaluation requires an eval_dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:3861\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3858\u001b[0m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[1;32m   3859\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m-> 3861\u001b[0m eval_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_eval_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fsdp_xla_v2_enabled:\n\u001b[1;32m   3863\u001b[0m     eval_dataloader \u001b[38;5;241m=\u001b[39m tpu_spmd_dataloader(eval_dataloader)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:965\u001b[0m, in \u001b[0;36mTrainer.get_eval_dataloader\u001b[0;34m(self, eval_dataset)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;124;03mReturns the evaluation [`~torch.utils.data.DataLoader`].\u001b[39;00m\n\u001b[1;32m    957\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;124;03m        If a `str`, will use `self.eval_dataset[eval_dataset]` as the evaluation dataset. If a `Dataset`, will override `self.eval_dataset` and must implement `__len__`. If it is a [`~datasets.Dataset`], columns not accepted by the `model.forward()` method are automatically removed.\u001b[39;00m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainer: evaluation requires an eval_dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    967\u001b[0m \u001b[38;5;66;03m# If we have persistent workers, don't do a fork bomb especially as eval datasets\u001b[39;00m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;66;03m# don't change during training\u001b[39;00m\n\u001b[1;32m    969\u001b[0m dataloader_key \u001b[38;5;241m=\u001b[39m eval_dataset \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(eval_dataset, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Trainer: evaluation requires an eval_dataset."
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.8291e+04, 8.0000e+00, 7.1100e+02, 3.0000e+00, 3.4410e+03, 9.0000e+00,\n",
       "         3.7380e+03, 4.4900e+02, 5.6400e+02, 1.3000e+01, 8.0000e+00, 8.2600e+02,\n",
       "         7.3300e+02, 1.0000e+01, 3.7000e+01, 1.5110e+03, 4.7000e+01, 1.0056e+04,\n",
       "         5.7000e+01, 3.0000e+00, 9.0000e+00, 5.2740e+03, 1.9611e+04, 6.0000e+00,\n",
       "         6.8000e+01, 2.1940e+03, 7.6000e+01, 2.6000e+01, 6.0000e+00, 3.0000e+00,\n",
       "         9.0000e+00, 3.0000e+00, 1.0656e+04, 9.2900e+03, 6.0000e+00, 6.9600e+03,\n",
       "         3.0000e+00, 9.0000e+00, 3.8700e+02, 7.5800e+02, 3.5800e+02, 2.4000e+01,\n",
       "         6.0240e+03, 8.0000e+00, 1.5183e+04, 5.0000e+00, 1.3470e+03, 1.9774e+04,\n",
       "         6.0000e+00, 3.0840e+03, 2.3000e+01, 6.0000e+00, 8.1600e+02, 3.4000e+01,\n",
       "         3.2900e+03, 2.2000e+01, 1.7000e+01, 3.6000e+01, 6.3100e+02, 6.0000e+00,\n",
       "         6.8000e+01, 2.1940e+03, 7.6000e+01, 2.6000e+01, 2.2000e+01, 7.0000e+00,\n",
       "         1.2950e+03, 1.9400e+03, 8.0000e+00, 1.5110e+03, 1.9000e+02, 8.0000e+00,\n",
       "         5.3620e+03, 6.0000e+00, 1.1000e+01, 8.0000e+00, 3.5800e+02, 1.6320e+03,\n",
       "         8.0100e+02, 3.8000e+01, 4.5800e+02, 6.8300e+02, 1.0630e+03, 2.6000e+01,\n",
       "         2.2000e+01, 7.0000e+00, 1.5480e+03, 7.0000e+00, 5.0000e+00, 2.2000e+01,\n",
       "         1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         325, 173,   9,   1]], device='cuda:0')\n",
      "Laila\n"
     ]
    }
   ],
   "source": [
    "task = \"Guess the main chaaracter name of the following story:\\n\"\n",
    "story = \"The rare manuscript was stolen, and Laila, a book collector, was found to be behind the theft. Her assistant, Tariq, uncovered the crime, but Laila’s connections in the literary world allowed her to avoid prosecution. Her name became infamous in the book industry.\"\n",
    "prompt = task + story\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "\n",
    "\n",
    "output_ids = model.generate(input_ids, num_beams=2)\n",
    "# Decode the generated output back into text\n",
    "print(output_ids)\n",
    "generated_story = tokenizer.decode(output_ids[0].cpu().tolist(), skip_special_tokens=True)\n",
    "\n",
    "# Print the generated story\n",
    "print(generated_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
