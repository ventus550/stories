{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import EncoderDecoderModel\n",
    "\n",
    "# model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\", \"gpt2\")\n",
    "# encoder = model.encoder\n",
    "# decoder = model.decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"stories.csv\")\n",
    "# Generate input-output pairs\n",
    "inputs = [\"Guess the main chaaracter name of the following story:\\n\" + row.story for _, row in df.iterrows()]\n",
    "outputs = df['name'].tolist()\n",
    "\n",
    "# Create a pandas DataFrame for easy conversion\n",
    "df_preprocessed = pd.DataFrame({'input_text': inputs, 'output_text': outputs})\n",
    "\n",
    "dataset = Dataset.from_pandas(df_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bigscience/mt0-small'  # You can change this to any T5 variant (t5-base, t5-large, etc.)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize both inputs and outputs\n",
    "    inputs = tokenizer(examples['input_text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    outputs = tokenizer(examples['output_text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    inputs['labels'] = outputs['input_ids']  # Use the tokenized output as labels for the model\n",
    "    return inputs\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_dataset = dataset.shuffle(seed=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Freeze encoder weights\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Initialize Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",            # output directory where model checkpoints will be saved\n",
    "    evaluation_strategy=\"steps\",       # Evaluate every epoch\n",
    "    learning_rate=5e-4,                # learning rate\n",
    "    per_device_train_batch_size=8,     # batch size for training\n",
    "    per_device_eval_batch_size=8,      # batch size for evaluation\n",
    "    num_train_epochs=10,                # number of training epochs\n",
    "    weight_decay=0.01,                 # strength of weight decay\n",
    "    logging_dir=\"./logs\",              # directory for storing logs\n",
    "    logging_steps=10,                 # log every 200 steps\n",
    "    save_steps=2000,                    # save model checkpoints every 500 steps\n",
    "    load_best_model_at_end=True,       # Load the best model when finished training\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the model to be trained\n",
    "    args=training_args,                  # training arguments\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=train_dataset,            # evaluation dataset\n",
    "    tokenizer=tokenizer,                 # tokenizer for preprocessing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "# trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"Guess the main chaaracter name of the following story:\\n\"\n",
    "story = \"The rare manuscript was stolen, and Rena, a book collector, was found to be behind the theft. Her assistant, Tariq, uncovered the crime, but Renaâ€™s connections in the literary world allowed her to avoid prosecution. Her name became infamous in the book industry.\"\n",
    "prompt = task + story\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "\n",
    "\n",
    "output_ids = model.generate(input_ids, num_beams=2)\n",
    "# Decode the generated output back into text\n",
    "print(output_ids)\n",
    "generated_story = tokenizer.decode(output_ids[0].cpu().tolist(), skip_special_tokens=True)\n",
    "\n",
    "# Print the generated story\n",
    "print(generated_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./model/mt0-small\")\n",
    "tokenizer.save_pretrained(\"./model/mt0-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
