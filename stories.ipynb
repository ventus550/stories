{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import EncoderDecoderModel\n",
    "\n",
    "# model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\", \"gpt2\")\n",
    "# encoder = model.encoder\n",
    "# decoder = model.decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import T5Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"stories.csv\")\n",
    "# Generate input-output pairs\n",
    "inputs = [\"Guess the main chaaracter name of the following story:\\n\" + row.story for _, row in df.iterrows()]\n",
    "outputs = df['name'].tolist()\n",
    "\n",
    "# Create a pandas DataFrame for easy conversion\n",
    "df_preprocessed = pd.DataFrame({'input_text': inputs, 'output_text': outputs})\n",
    "\n",
    "dataset = Dataset.from_pandas(df_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c30d6c7da24af8aa7c8930b445858a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 't5-small'  # You can change this to any T5 variant (t5-base, t5-large, etc.)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize both inputs and outputs\n",
    "    inputs = tokenizer(examples['input_text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    outputs = tokenizer(examples['output_text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    inputs['labels'] = outputs['input_ids']  # Use the tokenized output as labels for the model\n",
    "    return inputs\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_dataset = dataset.shuffle(seed=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Freeze encoder weights\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Initialize Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",            # output directory where model checkpoints will be saved\n",
    "    evaluation_strategy=\"steps\",       # Evaluate every epoch\n",
    "    learning_rate=5e-5,                # learning rate\n",
    "    per_device_train_batch_size=8,     # batch size for training\n",
    "    per_device_eval_batch_size=8,      # batch size for evaluation\n",
    "    num_train_epochs=3,                # number of training epochs\n",
    "    weight_decay=0.01,                 # strength of weight decay\n",
    "    logging_dir=\"./logs\",              # directory for storing logs\n",
    "    logging_steps=200,                 # log every 200 steps\n",
    "    save_steps=2000,                    # save model checkpoints every 500 steps\n",
    "    load_best_model_at_end=True,       # Load the best model when finished training\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the model to be trained\n",
    "    args=training_args,                  # training arguments\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    # eval_dataset=val_dataset,            # evaluation dataset\n",
    "    tokenizer=tokenizer,                 # tokenizer for preprocessing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdfcf935128f4333a5d51c24465973b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/189 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 54.1821, 'train_samples_per_second': 27.684, 'train_steps_per_second': 3.488, 'train_loss': 4.199990893167163, 'epoch': 3.0}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Trainer: evaluation requires an eval_dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:3861\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3858\u001b[0m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[1;32m   3859\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m-> 3861\u001b[0m eval_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_eval_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fsdp_xla_v2_enabled:\n\u001b[1;32m   3863\u001b[0m     eval_dataloader \u001b[38;5;241m=\u001b[39m tpu_spmd_dataloader(eval_dataloader)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:965\u001b[0m, in \u001b[0;36mTrainer.get_eval_dataloader\u001b[0;34m(self, eval_dataset)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;124;03mReturns the evaluation [`~torch.utils.data.DataLoader`].\u001b[39;00m\n\u001b[1;32m    957\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;124;03m        If a `str`, will use `self.eval_dataset[eval_dataset]` as the evaluation dataset. If a `Dataset`, will override `self.eval_dataset` and must implement `__len__`. If it is a [`~datasets.Dataset`], columns not accepted by the `model.forward()` method are automatically removed.\u001b[39;00m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainer: evaluation requires an eval_dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    967\u001b[0m \u001b[38;5;66;03m# If we have persistent workers, don't do a fork bomb especially as eval datasets\u001b[39;00m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;66;03m# don't change during training\u001b[39;00m\n\u001b[1;32m    969\u001b[0m dataloader_key \u001b[38;5;241m=\u001b[39m eval_dataset \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(eval_dataset, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Trainer: evaluation requires an eval_dataset."
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.8291e+04, 8.0000e+00, 7.1100e+02, 3.0000e+00, 3.4410e+03, 9.0000e+00,\n",
       "         3.7380e+03, 4.4900e+02, 5.6400e+02, 1.3000e+01, 8.0000e+00, 8.2600e+02,\n",
       "         7.3300e+02, 1.0000e+01, 3.7000e+01, 1.5110e+03, 4.7000e+01, 1.0056e+04,\n",
       "         5.7000e+01, 3.0000e+00, 9.0000e+00, 5.2740e+03, 1.9611e+04, 6.0000e+00,\n",
       "         6.8000e+01, 2.1940e+03, 7.6000e+01, 2.6000e+01, 6.0000e+00, 3.0000e+00,\n",
       "         9.0000e+00, 3.0000e+00, 1.0656e+04, 9.2900e+03, 6.0000e+00, 6.9600e+03,\n",
       "         3.0000e+00, 9.0000e+00, 3.8700e+02, 7.5800e+02, 3.5800e+02, 2.4000e+01,\n",
       "         6.0240e+03, 8.0000e+00, 1.5183e+04, 5.0000e+00, 1.3470e+03, 1.9774e+04,\n",
       "         6.0000e+00, 3.0840e+03, 2.3000e+01, 6.0000e+00, 8.1600e+02, 3.4000e+01,\n",
       "         3.2900e+03, 2.2000e+01, 1.7000e+01, 3.6000e+01, 6.3100e+02, 6.0000e+00,\n",
       "         6.8000e+01, 2.1940e+03, 7.6000e+01, 2.6000e+01, 2.2000e+01, 7.0000e+00,\n",
       "         1.2950e+03, 1.9400e+03, 8.0000e+00, 1.5110e+03, 1.9000e+02, 8.0000e+00,\n",
       "         5.3620e+03, 6.0000e+00, 1.1000e+01, 8.0000e+00, 3.5800e+02, 1.6320e+03,\n",
       "         8.0100e+02, 3.8000e+01, 4.5800e+02, 6.8300e+02, 1.0630e+03, 2.6000e+01,\n",
       "         2.2000e+01, 7.0000e+00, 1.5480e+03, 7.0000e+00, 5.0000e+00, 2.2000e+01,\n",
       "         1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         325, 173,   9,   1]], device='cuda:0')\n",
      "Laila\n"
     ]
    }
   ],
   "source": [
    "task = \"Guess the main chaaracter name of the following story:\\n\"\n",
    "story = \"The rare manuscript was stolen, and Laila, a book collector, was found to be behind the theft. Her assistant, Tariq, uncovered the crime, but Lailaâ€™s connections in the literary world allowed her to avoid prosecution. Her name became infamous in the book industry.\"\n",
    "prompt = task + story\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "\n",
    "\n",
    "output_ids = model.generate(input_ids, num_beams=2)\n",
    "# Decode the generated output back into text\n",
    "print(output_ids)\n",
    "generated_story = tokenizer.decode(output_ids[0].cpu().tolist(), skip_special_tokens=True)\n",
    "\n",
    "# Print the generated story\n",
    "print(generated_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
